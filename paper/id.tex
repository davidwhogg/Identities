\documentclass[12pt]{article}
\usepackage{url}

\addtolength{\textheight}{1.5in}
\addtolength{\headheight}{-0.75in}
\sloppy\sloppypar\raggedbottom\frenchspacing

\begin{document}

\section*{Matrix and Gaussian Identities}

{\raggedright
\textbf{Sam~Roweis}%
\footnote{Deceased.
Formerly at the \textsl{Department of Computer Science, New York University};
and \textsl{Google Inc.};
and the \textsl{Department of Computer Science, University of Toronto}.},
\textbf{David~W.~Hogg}%
\footnote{\textsl{Center for Cosmology and Particle Physics, Department of Physics, New York University};
and the \textsl{Flatiron Institute, a Division of the Simons Foundation};
and the \textsl{Max-Planck-Insitut f\"ur Astronomie}.},
\textbf{Dustin~Lang}%
\footnote{\textsl{Perimeter Institute}.},
\& \textbf{Boris~Leistedt}%
\footnote{\textsl{Center for Cosmology and Particle Physics, Department of Physics, New York University}.}
}

\paragraph{Abstract:}
Across all areas of data analysis, probability, statistics, machine learning, and
indeed a far larger set of domains, the linear algebra of rectangular matrices
is core. And nowhere is this more true than in problems that involve Gaussians (normal
distributions), which appear explicitly or implicitly in many different methods.
Here we assemble a set of mathematical identities and relationships involving
matrices---including scalar, vector, and matrix forms constructed from combinations of
scalars, vectors, and matrices---and their derivatives.
We also assemble a set of identies involving Gaussians.
These sets of identities are not intended to be exhaustive.
Instead we concentrate on the identities most valuable
for the appearance of matrices and Gaussians in machine-learning and data-analysis contexts.
This paper expands and adds context to some crib sheets that have been
available on the internet for years.

\clearpage
\section{Introduction}

It is possible to take an entire undergraduate linear algebra course
without ever performing any operations on any non-square
matrices.\footnote{One of us---Hogg---did!}
And yet non-square matrices are the norm in matters of data analysis
and machine learning.
Consider the concept of rectangular data, or low-rank forms for
square matrices.
Or a derivative of one vector with respect to another of different
dimensionality.
In general, a matrix represents a kind of ratio of vectors; if those
vectors are in different spaces then the matrix will, in general, be
non-square.
And both square and non-square matrices are involved in a wide range
of scalar, vector, and tensor forms that appear in diverse contexts in
machine learning and probabilistic modeling.

Linear algebra is the language of machine learning.
The manipulation of linear algebra expressions is a key capability for
any researcher in the field.
These notes (below) are intended to help with the development and
propagation of that capability.

In our own research (which tends towards probabilistic modeling and
probabilistic methods), Gaussians---normal distributions---also appear
everywhere, in part because they are so simple in their properties
(as we will see below), and in part because
they are the outcome of the central limit theorem, and in part
because they are maximum-entropy distributions (constrained by
a known mean and variance).
The normal distribution involves the exponentiation of a
non-positive semi-definite quadratic form.
The general non-positive semi-definite quadratic form involves linear
algebra:
It is the inner product of a vector with itself through (the negative
of) a non-negative semi-definite tensor, or metric tensor, which
itself might have low rank or interesting structure.
For this reason, linear algebra is critical to manipulation of
interesting Gaussian forms, and there are shared technologies between
linear algebra and probabilistic inference.

These two sets of identities---one for the manipulation and
simplification of matrix expressions, and one for the manipulation and
simplification of Gaussian expressions---were prepared many years ago
by Sam Roweis, when he was a graduate
student at the California Institute of Technology.
They have remained on the internet after Roweis's untimely death in 2010,
as a valuable tool for researchers, and as a reminder of Roweis for his
friends.
Roweis was one of the pioneers of probabilistic machine learning, an ambitious
applied researcher, and a wonderful colleague and friend.
Anything more is out of scope, but we encourage anyone using these notes to
take a look at his remarkable body of work.

There is nothing new or original in these notes; nothing that can't be found
elsewhere (CITE MATRIX COOKBOOK, for example)... HOGG ADD WORDS.

In what follows, we include the Roweis notes verbatim, preserving
the original typesetting decisions as closely as possible.
We have not, however, preserved exactly the equation numbering,
because if we did there would be name collisions between the matrix identities
and the Gaussian identities.
We have also corrected a few tiny typos, including one or two places where
Roweis (deliberately) didn't capitalize letters.
We make these changes with apologies to the faithful and utmost respect to the dead.
For those who want to see the original notes, we have preserved them in a git
repository available online\footnote{\url{https://github.com/davidwhogg/Identities}}.
We follow the verbatim notes with some discussion and some additional forms.

\clearpage
\section{Matrix Identities}

Hello World

\clearpage
\section{Gaussian Identities}

Hello World

\clearpage
\section{Discussion}

Hello World.

\end{document}
