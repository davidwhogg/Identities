% This file is part of the Identities project.
% Copyrignt 1999, 2019 the authors.

% to-do:
% ------
% - what's our audience? Students and postdocs working in probabilistic models?
% - should we state our imagination of the audience explictly?
% - analyze this difference:
%   diff roweis/Teaching/Tutorials/GCNUTut/matrixid.tex roweis/Teaching/TNotes/MatID/matrixid.tex 
% - write discussion
% - finish introduction

\documentclass[12pt]{article}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{url}

\addtolength{\textheight}{1.5in}
\addtolength{\headheight}{-0.75in}
\sloppy\sloppypar\raggedbottom\frenchspacing

\begin{document}\thispagestyle{empty}

\section*{Matrix and Gaussian Identities}

{\raggedright
\textbf{Sam~Roweis}%
\footnote{Deceased.
Formerly at the \textsl{Department of Computer Science, New York University};
and \textsl{Google Inc.};
and the \textsl{Department of Computer Science, University of Toronto}.},
\textbf{David~W.~Hogg}%
\footnote{\textsl{Center for Cosmology and Particle Physics, Department of Physics, New York University};
and the \textsl{Flatiron Institute, a Division of the Simons Foundation};
and the \textsl{Max-Planck-Insitut f\"ur Astronomie}.},
\textbf{Dustin~Lang}%
\footnote{\textsl{Perimeter Institute}.},
\& \textbf{Boris~Leistedt}%
\footnote{\textsl{Center for Cosmology and Particle Physics, Department of Physics, New York University}.}
}

\paragraph{Abstract:}
Across all areas of data analysis, probability, statistics, machine learning, and
indeed a far larger set of domains, the linear algebra of rectangular matrices
is core. And nowhere is this more true than in problems that involve Gaussians (normal
distributions), which appear explicitly or implicitly in many different methods.
Here we assemble a set of mathematical identities and relationships involving
matrices---including scalar, vector, and matrix forms constructed from combinations of
scalars, vectors, and matrices---and their derivatives.
We also assemble a set of identies involving Gaussians.
These sets of identities are not intended to be exhaustive.
Instead we concentrate on the identities most valuable
for the appearance of matrices and Gaussians in machine-learning and data-analysis contexts.
This paper expands and adds context to some crib sheets that have been
available on the internet for years.

\clearpage
\section{Introduction}

It is possible to take an entire undergraduate linear algebra course
without ever performing any operations on any non-square
matrices.\footnote{One of us---Hogg---did!}
And yet non-square matrices are the norm in matters of data analysis
and machine learning.
Consider the concept of rectangular data, or low-rank forms for
square matrices.
Or a derivative of one vector with respect to another of different
dimensionality.
In general, a matrix represents a kind of ratio of vectors; if those
vectors are in different spaces then the matrix will, in general, be
non-square.
And both square and non-square matrices are involved in a wide range
of scalar, vector, and tensor forms that appear in diverse contexts in
machine learning and probabilistic modeling.

Linear algebra is the language of machine learning.
The manipulation of linear algebra expressions is a key capability for
any researcher in the field.
These notes (below) are intended to help with the development and
propagation of that capability.

In our own research (which tends towards probabilistic modeling and
probabilistic methods), Gaussians---normal distributions---also appear
everywhere, in part because they are so simple in their properties
(as we will see below), and in part because
they are the outcome of the central limit theorem, and in part
because they are maximum-entropy distributions (constrained by
a known mean and variance).
The normal distribution involves the exponentiation of a
non-positive semi-definite quadratic form.
The general non-positive semi-definite quadratic form involves linear
algebra:
It is the inner product of a vector with itself through (the negative
of) a non-negative semi-definite tensor, or metric tensor, which
itself might have low rank or interesting structure.
For this reason, linear algebra is critical to manipulation of
interesting Gaussian forms, and there are shared technologies between
linear algebra and probabilistic inference.

These two sets of identities---one for the manipulation and
simplification of matrix expressions, and one for the manipulation and
simplification of Gaussian expressions---were prepared many years ago
by Sam Roweis, when he was a graduate
student at the California Institute of Technology.
The versions of his notes preserved here were dated (unreliably) at summer
of 1999.
They have remained on the internet after Roweis's untimely death in 2010,
as a valuable tool for researchers, and as a reminder of Roweis for his
friends.
Roweis was one of the pioneers of probabilistic machine learning, an ambitious
applied researcher, and a wonderful colleague and friend.
Anything more is out of scope, but we encourage anyone using these notes to
take a look at his remarkable body of work.

There is nothing new or original in these notes.
They contain nothing that can't be found elsewhere (for example,
essentially all of the matrix identities given here are also in
\textit{The Matrix Cookbook}\footnote{%
\url{http://www2.imm.dtu.dk/pubdb/views/publication_details.php?id=3274}},
Petersen \& Pedersen, 2012).
These lists of identities represent a subjective and (no doubt) flawed
distillation by Roweis down to what's most important for machine
learning and probabilistic inference.
They also represent a snapshot of the young Sam Roweis, which we would
like to preserve for sentimental reasons.

In what follows, we include the Roweis notes verbatim, preserving
the original typesetting decisions as closely as possible.
We have not, however, preserved the equation numbering,
and we have messed with the line breaks or horizontal alignment for some equations.
We have also corrected a few tiny typos, including one or two places where
Roweis (deliberately) didn't capitalize letters (though we left most of it as-is).
We make these changes with apologies to the faithful and respect for the dead.
For those who want to see the original notes, we have preserved them in a git
repository available online\footnote{\url{https://github.com/davidwhogg/Identities}}.
We follow the verbatim notes with some discussion and some additional forms.

\clearpage
\section{Matrix Identities}

%% \documentclass[11pt,reqno,intlimits]{article}
%% \usepackage{amsmath}
%% \usepackage{amssymb}
%% \parindent=0in

\newcommand{\ie}{\text{i.e.~}}
\newcommand{\eg}{\text{e.g.~}}

\newcommand{\distrib}{\thicksim}
\newcommand{\normal}[2]{\mathcal{N} \left( #1,#2 \right)}
\newcommand{\normale}[3]{\mathcal{N} \left( #1,#2 \right) |_{#3}}
\newcommand{\prob}[1]{P \left( #1 \right)}
\newcommand{\by}{\times}

\newcommand{\sm}[1]{[\begin{smallmatrix}#1\end{smallmatrix}]}
\newcommand{\bm}[1]{\begin{bmatrix}#1\end{bmatrix}}
\newcommand{\xx}{{\mathbf x}}
\newcommand{\yy}{{\mathbf y}}
\newcommand{\zz}{{\mathbf z}}
\newcommand{\mm}{{\mathbf m}}
\renewcommand{\AA}{{\mathbf A}}
\newcommand{\BB}{{\mathbf B}}
\newcommand{\CC}{{\mathbf C}}
\newcommand{\DD}{{\mathbf D}}
\newcommand{\EE}{{\mathbf E}}
\newcommand{\FF}{{\mathbf F}}
\newcommand{\XX}{{\mathbf X}}
\newcommand{\YY}{{\mathbf Y}}
\newcommand{\VV}{{\mathbf V}}
\newcommand{\UU}{{\mathbf U}}
\newcommand{\WW}{{\mathbf W}}
\newcommand{\QQ}{{\mathbf Q}}
\newcommand{\RR}{{\mathbf R}}
\renewcommand{\SS}{{\mathbf S}}
\newcommand{\TT}{{\mathbf T}}
\newcommand{\ZZ}{{\mathbf 0}}
\newcommand{\II}{{\mathbf I}}
\newcommand{\ee}{{\mathbf e}}
\newcommand{\bb}{{\mathbf b}}
\newcommand{\cc}{{\mathbf c}}
\newcommand{\aaa}{{\mathbf a}}
%\newcommand{\bbb}{{\boldsymbol \beta}}
\newcommand{\mmm}{{\boldsymbol \mu}}
\newcommand{\Lm}{{\boldsymbol \Lambda}}
\newcommand{\Sm}{{\boldsymbol \Sigma}}
\newcommand{\Pv}{{\boldsymbol \pi}_1}
\newcommand{\Pm}{{\boldsymbol \Pi}_1}
\newcommand{\trace}[1]{\text{Tr}\left[ #1 \right]}
\newcommand{\rank}[1]{\text{rank}\left[ #1 \right]}

%% \begin{document}

%% \title{matrix identities}
%% %\author{sam roweis}
%% \date{ \hrule
%% %\begin{center}
%% %(revised June 1999)
%% %\end{center}
%% }

%% \maketitle

\noindent \textbf{note}
that $\aaa$,$\bb$,$\cc$ and $\AA$,$\BB$,$\CC$ 
do not depend on $\XX$,$\YY$,$\xx$,$\yy$ or $z$

\subsection{basic formulae}
\begin{align}
\AA(\BB + \CC) & = \AA\BB + \AA \CC \\
(\AA + \BB)^T & = \AA^T + \BB^T \\
(\AA\BB)^T & = \BB^T \AA^T \\
\text{if individual inverses exist:} \nonumber\\
(\AA\BB)^{-1} & = \BB^{-1}\AA^{-1} \\
(\AA^{-1})^T & = (\AA^T)^{-1}
\end{align}

\subsection{trace, determinant and rank}
\begin{align}
|\AA \BB| & = |\AA| |\BB| \\
|\AA^{-1}| & = \frac{1}{|\AA|} \\
|\AA| & = \prod \text{evals} \\
\trace{\AA} & = \sum \text{evals} \\
\text{if the cyclic products are well defined:} \nonumber\\
\trace{\AA\BB\CC \dots}   = \trace{\BB\CC \dots \AA}
                        & = \trace{\CC \dots \AA\BB}
                          = \dots \\
\rank{\AA}   = \rank{\AA^T\AA}
           & = \rank{\AA\AA^T}
\end{align}
\begin{align}
\text{condition number} & = \sqrt{\frac{\text{biggest
eigenvalue}}{\text{smallest eigenvalue}}}
\end{align}

\subsection{derivatives of traces}
\noindent \textbf{derivatives}
of scalar forms with respect to scalars, vectors, or
matricies are indexed in the obvious way. similarly, the indexing for
derivatives of vectors and matrices with respect to scalars is
straightforward.

\begin{align}
\frac{\partial \trace{\XX}}{\partial \XX} &= \II \\
\frac{\partial \trace{\XX \AA}}{\partial \XX} =
\frac{\partial \trace{\AA \XX}}{\partial \XX} &= \AA^T\\
\frac{\partial \trace{\XX^T \AA}}{\partial \XX} =
\frac{\partial \trace{\AA \XX^T}}{\partial \XX} &= \AA\\
\frac{\partial \trace{\XX^T\AA \XX}}{\partial \XX} &= (\AA+\AA^T)\XX \\
\frac{\partial \trace{\XX^{-1} \AA}}{\partial \XX} &= 
-\XX^{-1} \AA^T \XX^{-1}
\end{align}

\subsection{derivatives of determinants}
\begin{align}
\frac{\partial |\AA\XX\BB|}{\partial \XX} = |\AA\XX\BB|(\XX^{-1})^T &=
|\AA\XX\BB|(\XX^T)^{-1}\\
\frac{\partial \ln |\XX|}{\partial \XX} = (\XX^{-1})^T &= (\XX^T)^{-1}\\
\frac{\partial \ln |\XX(z)|}{\partial z} &=
\trace{(\XX^{-1})^T\frac{\partial \XX}{\partial z}} \\
\text{for real, square} \thinspace \AA \quad 
\frac{\partial |\XX^T\AA\XX|}{\partial \XX} &=
|\XX^T\AA\XX|(\AA+\AA^T)\XX(\XX^T\AA\XX)^{-1} 
\end{align}

\subsection{derivatives of scalar forms}
\begin{align}
\frac{\partial (\aaa^T \xx)}{\partial \xx} = 
\frac{\partial (\xx^T \aaa)}{\partial \xx} &= \aaa \\
\frac{\partial (\xx^T\AA\xx)}{\partial \xx} &= (\AA + \AA^T)\xx \\ 
\frac{\partial (\aaa^T\XX\bb)}{\partial \XX} &= \aaa\bb^T  \\
\frac{\partial (\aaa^T\XX^T\bb)}{\partial \XX} &= \bb\aaa^T  \\
\frac{\partial (\aaa^T\XX\aaa)}{\partial \XX} = 
\frac{\partial (\aaa^T\XX^T\aaa)}{\partial \XX} &= \aaa\aaa^T \\
\frac{\partial (\aaa^T\XX^T\CC\XX\bb)}{\partial \XX} &= 
\CC^T\XX\aaa\bb^T + \CC\XX\bb\aaa^T \\
\frac{\partial \left((\XX\aaa+\bb)^T\CC(\XX\aaa+\bb)\right)}{\partial \XX} &= 
(\CC+\CC^T)(\XX\aaa+\bb)\aaa^T
\end{align}

%d                      -1 dA   
%-- log det A = Trace (A   --), 
%da                        da   

\noindent
the {\bf derivative} of one vector $\yy$ with respect to
another vector $\xx$ is a matrix whose $(i,j)^{th}$ element is
$\partial y(j)/ \partial x(i)$. such a derivative should be written as
$\partial \yy^T / \partial \xx$
 in which case it is the {\em
Jacobian} matrix of $\yy$ wrt $\xx$. its determinant 
represents the ratio of the hypervolume $d\yy$ to that of
$d\xx$ so that 
$\int f(\yy) d\yy = \int f(\yy(\xx)) |\partial \yy^T /\partial \xx|d\xx$. 
however, the sloppy forms 
$\partial \yy / \partial \xx$,
$\partial \yy^T / \partial \xx^T$ and
$\partial \yy / \partial \xx^T$ are often used for this Jacobian matrix.

\subsection{derivatives of vector/matrix forms}
\begin{align}
\frac{\partial (\XX^{-1})}{\partial z} &= -
\XX^{-1}  \frac{\partial \XX}{\partial z} \XX^{-1} \\
\frac{\partial (\AA \xx)}{\partial z} &= \AA  
\frac{\partial \xx}{\partial z}\\
\frac{\partial (\XX \YY)}{\partial z} &=
\XX  \frac{\partial \YY}{\partial z} +
\frac{\partial \XX}{\partial z} \YY \\ 
\frac{\partial (\AA \XX \BB)}{\partial z} &= \AA 
\frac{\partial \XX}{\partial z} \BB\\  
\frac{\partial (\xx^T\AA)}{\partial \xx} &= \AA \\ 
\frac{\partial (\xx^T)}{\partial \xx} &= \II \\
\frac{\partial (\xx^T\AA\xx\xx^T)}{\partial \xx} &= 
(\AA+\AA^T)\xx\xx^T+\xx^T\AA\xx\II
\end{align}

\subsection{constrained maximization}
the maximum over $\xx$ of the quadratic form:
\begin{align}
\mmm^T\xx - \frac{1}{2}\xx^T\AA^{-1}\xx
\end{align}
subject to the $J$ conditions $c_j(\xx)=0$  is given by:
\begin{align}
\AA\mmm + \AA\CC\Lm, \qquad \Lm & = -4(\CC^T\AA\CC)\CC^T\AA\mmm
\end{align}
where the $j$th column of $\CC$ is $\partial c_j(\xx) / \partial \xx$

\subsection{symmetric matrices}
have real eigenvalues, though perhaps not distinct and can always
be diagonalized to the form: 
\begin{align}
\AA & = \CC \Lm \CC^T
\end{align}
where the columns of $\CC$
are (orthonormal) eigenvectors (\ie $\CC \CC^T = \II$) and the
diagonal of $\Lm$ has the eigenvalues

\subsection{block matrices}
for conformably partitioned block matrices, addition and
multiplication is performed by adding and multiplying blocks in
exactly the same way as scalar elements of regular matrices

\noindent 
however, determinants and inverses of block matrices are very tricky;
for 2 blocks by 2 blocks the results are:
\begin{align}
\begin{vmatrix}\AA_{11}&\AA_{12}\\ \AA_{21}&\AA_{22}\end{vmatrix} 
&= |\AA_{22}| \cdot |\FF_{11}| = |\AA_{11}| \cdot  |\FF_{22}| \\ 
{\begin{bmatrix}\AA_{11}&\AA_{12}\\ \AA_{21}&\AA_{22}\end{bmatrix}}^{-1} &=
\begin{bmatrix}
\FF_{11}^{-1}&-\AA_{11}^{-1}\AA_{12}\FF_{22}^{-1}\\
-\FF_{22}^{-1}\AA_{21}\AA_{11}^{-1}& \FF_{22}^{-1}
\end{bmatrix} \\
&= \begin{bmatrix} 
\AA_{11}^{-1}+\AA_{11}^{-1} \AA_{12} \FF_{22}^{-1} \AA_{21} \AA_{11}^{-1} & 
-\FF_{11}^{-1} \AA_{12} \AA_{22}^{-1}\\
-\AA_{22}^{-1} \AA_{21} \FF_{11}^{-1} & 
\AA_{22}^{-1}+\AA_{22}^{-1} \AA_{21} \FF_{11}^{-1} \AA_{12} \AA_{22}^{-1}
\end{bmatrix} 
\end{align}
where
\begin{align}
\FF_{11} & = \AA_{11}-\AA_{12}\AA_{22}^{-1}\AA_{21} \\
\FF_{22} & = \AA_{22}-\AA_{21}\AA_{11}^{-1}\AA_{12}
\end{align}
for block {\em diagonal} matrices things are much easier:
\begin{align}
\begin{vmatrix}\AA_{11} & \ZZ \\ \ZZ & \AA_{22}\end{vmatrix} 
&=  |\AA_{11}||\AA_{22}|\\
{\begin{bmatrix}\AA_{11} & \ZZ \\ \ZZ &\AA_{22}\end{bmatrix}}^{-1} &=
\begin{bmatrix}
\AA_{11}^{-1} & \ZZ \\ \ZZ & \AA_{22}^{-1}
\end{bmatrix}
\end{align}

\subsection{matrix inversion lemma}
using the above results for block matrices we can make some
substitutions and get the following important result:
\begin{align}
(\AA + \XX\BB\XX^T)^{-1} & = 
\AA^{-1}-\AA^{-1}\XX(\BB^{-1}+\XX^T\AA^{-1}\XX)^{-1}\XX^T\AA^{-1}
\end{align}
where $\AA$ and $\BB$ are {\em square} and {\em invertible} matrices
but need not be of the same dimension.
this lemma often
allows a really hard inverse to be converted into an easy inverse. the
most typical example of this is when $\AA$ is large but diagonal, and
$\XX$ has many rows but few columns

\clearpage
\section{Gaussian Identities}



\subsection{multidimensional gaussian}
a $d$-dimensional multidimensional gaussian (normal) density for $\xx$
is:
\begin{align}
\normal{\mmm}{\Sm} = 
(2\pi)^{-d/2}|\Sm|^{-1/2}
\exp\left[-\frac{1}{2}(\xx-\mmm)^T\Sm^{-1}(\xx-\mmm)\right]
\end{align}
it has entropy:
\begin{align}
S = \frac{1}{2}\log_2\left[ (2\pi e)^d|\Sm| \right] \: - \text{const}
\quad \text{bits}
\end{align}
% entropy of a gaussian is
% S = k/2 (1+log(2 pi) ) + 1/2 log ( m^2 det A^{-1} ) 
% where k is dimensionality, A is covariance matrix and m is the
% measure on \xx that makes the probability dimensionless
where $\Sm$ is a symmetric postive semi-definite covariance matrix and
the (unfortunate) constant is the log of the units in which $\xx$ is
measured over the ``natural units''

\subsection{linear functions of a normal vector}
no matter how $\xx$ is distributed,
\begin{align}
\mathrm{E}[\AA\xx+\yy] &= \AA(\mathrm{E}[\xx]) + \yy \\
\mathrm{Covar}[\AA\xx + \yy] &= \AA(\mathrm{Covar}[\xx])\AA^T
\end{align}
in particular this means that for normal distributed quantities:
\begin{align}
\xx \distrib \normal{\mmm}{\Sm} &\Rightarrow
(\AA\xx + \yy) \distrib \normal{\AA\mmm + \yy}{\AA \Sm \AA^T} \\
\xx \distrib \normal{\mmm}{\Sm} &\Rightarrow
\Sm^{-1/2}(\xx - \mmm) \distrib \normal{\ZZ}{\II} \\
\xx \distrib \normal{\mmm}{\Sm} &\Rightarrow
(\xx - \mmm)^T\Sm^{-1}(\xx - \mmm) \distrib \chi^2_n 
\end{align}

\subsection{marginal and conditional distributions}
let the vector $\zz=[\xx^T \yy^T]^T$ be normally distributed according
to:
\begin{align}
\zz  = \begin{bmatrix}\xx \\ \yy \end{bmatrix}
\distrib \normal{\begin{bmatrix}\aaa \\ \bb \end{bmatrix}}
{\begin{bmatrix}\AA & \CC \\ \CC^T & \BB \end{bmatrix}}
\end{align}
where $\CC$ is the (non-symmetric)
cross-covariance matrix between $\xx$ and $\yy$ which has as
many rows as the size of $\xx$ and as many columns as the size of
 $\yy$. then the marginal distributions are:
\begin{align}
\xx &\distrib \normal{\aaa}{\AA} \\
\yy &\distrib \normal{\bb}{\BB}
\end{align}
and the conditional distributions are:
\begin{align}
\xx | \yy &\distrib \normal{\aaa + \CC\BB^{-1}(\yy-\bb)}
{\AA - \CC\BB^{-1}\CC^T} \\
\yy | \xx &\distrib \normal{\bb + \CC^T\AA^{-1}(\xx-\aaa)}
{\BB - \CC^T\AA^{-1}\CC}
\end{align}

\subsection{multiplication}
the multiplication of two gaussian functions is another gaussian
function (although no longer normalized). in particular,
\begin{align}
\normal{\aaa}{\AA} \cdot \normal{\bb}{\BB} \propto \normal{\cc}{\CC}
\end{align} 
where
\begin{align}
\CC &= \left( \AA^{-1}+\BB^{-1} \right)^{-1}\\
\cc &= \CC\AA^{-1}\aaa + \CC\BB^{-1}\bb
\end{align}
amazingly, the normalization constant $z_c$ is Gaussian in either
$\aaa$ or $\bb$:
\begin{align}
z_c &= (2\pi)^{-d/2}|\CC|^{+1/2}|\AA|^{-1/2}|\BB|^{-1/2}
\exp \left[ -\frac{1}{2}( \aaa^T\AA^{-1}\aaa + \bb^T\BB^{-1}\bb 
- \cc^T\CC^{-1}\cc ) \right]\\
z_c(\aaa) &\distrib \normal{(\AA^{-1}\CC\AA^{-1})^{-1}
(\AA^{-1}\CC\BB^{-1})\bb}
{(\AA^{-1}\CC\AA^{-1})^{-1}}\\
z_c(\bb) &\distrib \normal{(\BB^{-1}\CC\BB^{-1})^{-1}
(\BB^{-1}\CC\AA^{-1})\aaa}
{(\BB^{-1}\CC\BB^{-1})^{-1}}
\end{align}

\subsection{quadratic forms}
the expectation of a quadratic form under a gaussian is another
quadratic form (plus an annoying constant). in particular,
if $\xx$ is gaussian distributed with mean $\mm$ and variance $\SS$ then,
\begin{align}
\int_{\xx}(\xx-\mmm)^T\Sm^{-1}(\xx-\mmm)\normal{\mm}{\SS}d\xx \\
= (\mmm-\mm)^T\Sm^{-1}(\mmm-\mm)+\trace{\Sm^{-1}\SS}
\end{align}
if the original quadratic form has a linear function of $\xx$ the
result is still simple:
\begin{align}
\int_{\xx}(\WW\xx-\mmm)^T\Sm^{-1}(\WW\xx-\mmm)\normal{\mm}{\SS}d\xx \\
= (\mmm-\WW\mm)^T\Sm^{-1}(\mmm-\WW\mm)+\trace{\WW^T\Sm^{-1}\WW\SS}
\end{align}

\subsection{convolution}
the convolution of two gaussian functions is another gaussian function
(although no longer normalized). in particular, 
\begin{align}
\normal{\aaa}{\AA} \ast \normal{\bb}{\BB} \propto \normal{\aaa+\bb}{\AA+\BB}
\end{align}
this is a direct consequence of the fact that the Fourier transform of
a gaussian is another gaussian and that the multiplication of two
gaussians is still gaussian.

\subsection{Fourier transform}
the (inverse)Fourier transform of a gaussian function is another gaussian
function (although no longer normalized). in particular,
\begin{align}
\mathcal{F}\left[\normal{\aaa}{\AA}\right] &\propto
\normal{j\AA^{-1}\aaa}{\AA^{-1}}\\
\mathcal{F}^{-1}\left[\normal{\bb}{\BB}\right] &\propto
\normal{-j\BB^{-1}\bb}{\BB^{-1}}
\end{align}
where $j=\sqrt{-1}$

\subsection{constrained maximization}
the maximum over $\xx$ of the quadratic form:
\begin{align}
\mmm^T\xx - \frac{1}{2}\xx^T\AA^{-1}\xx
\end{align} 
subject to the $J$ conditions $c_j(\xx)=0$  is given by:
\begin{align}
\AA\mmm + \AA\CC\Lm, \qquad \Lm = -4(\CC^T\AA\CC)\CC^T\AA\mmm
\end{align}
where the $j$th column of $\CC$ is $\partial c_j(\xx) / \partial \xx$

\clearpage
\section{Discussion}

Hello World.

\clearpage
\section*{References}
\begin{list}{}{%
    \rightmargin=0in
    \leftmargin=\parindent
    \topsep=0ex
    \partopsep=0pt
    \itemsep=0.2ex
    \parsep=0pt
    \itemindent=-1.0\leftmargin
    \listparindent=0.0\leftmargin}
\item Petersen,~K.~B. \& Pedersen,~M.~S., 2012, The Matrix Cookbook (version 20121115)
\url{http://www2.imm.dtu.dk/pubdb/views/publication_details.php?id=3274}.
\end{list}

\end{document}
